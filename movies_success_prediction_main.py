# -*- coding: utf-8 -*-
"""Movies Success Prediction main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dUI57ACDWDTwFT_oxFX5GLxWn7jrzcOq
"""

from google.colab import drive
drive.mount('/content/drive/',force_remount=True)

"""**Neccessary Packages installation**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn as skn
!pip install sklearn
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from scipy import stats
from ast import literal_eval
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
from sklearn.impute import SimpleImputer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet

"""**Retrieving the dataset from google drive**"""

movies_data = pd.read_csv('/content/drive/MyDrive/movies/movie_success_rate.csv')

movies_data

movies_data.shape

"""**Some Visualization of Dataset**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import seaborn as sns

a4_dims = (11.7, 8.27)

fig, ax = plt.subplots(figsize=a4_dims)
sns.stripplot(x="Year", y="Rating", data=movies_data, jitter=True);
plt.title('Rating BASED ON YEAR')
plt.show

a5 = (50, 40)
fig, ax = plt.subplots(figsize=a4_dims)
sns.stripplot(x="Year", y="Votes", data=movies_data, jitter=True);
plt.title('Votes BASED ON Years')
plt.show

fig, ax = plt.subplots(figsize=a4_dims)
sns.stripplot(x="Year", y="Revenue (Millions)", data=movies_data, jitter=True);
plt.title('Revenue BASED ON Years')
plt.show

"""**Dropping Unnecessary Columns**"""

movies_data=movies_data.drop('Title',axis=1)
movies_data=movies_data.drop('Genre',axis=1)
movies_data=movies_data.drop('Description',axis=1)
movies_data=movies_data.drop('Director',axis=1)
movies_data=movies_data.drop('Actors',axis=1)
movies_data

"""Checking if there has any null data in the dataset"""

movies_data.isnull().any()

"""As there has null data, we need to take care of it"""

movies_data = movies_data.fillna(movies_data.median())

movies_data.isnull().any()

"""Then we have checkedour decision or label column - Success"""

movies_data.Success.value_counts()

sns.countplot(movies_data['Success'])

"""As the success and unsuccess numbers are not same we will try to make it close to equal"""

from sklearn.utils import resample
#create two different dataframe of majority and minority class 
df_majority = movies_data[(movies_data['Success']==0)] 
df_minority = movies_data[(movies_data['Success']==1)] 
# upsample minority class
df_minority_upsampled = resample(df_minority, 
                                 replace=True,    # sample with replacement
                                 n_samples= (690+149), # to match majority class
                                 random_state=42)  # reproducible results
# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_minority_upsampled, df_majority])
df_upsampled

df_upsampled.Success.value_counts()

sns.countplot(df_upsampled['Success'])

"""But later we had realised that without doing above process we get realistic prediction. So we decide not to use the newly merged success and unsuccess column and use the previous dataset.

Now, we will check the correlation among the columns, which we will use later.
"""

mov_check_2 =movies_data.corr()
mov_check_2.style.background_gradient(cmap="coolwarm")

plt.figure(figsize=(50,60))
sns.heatmap(mov_check_2, annot=True)

"""From the correlation table we could determine that the columns Runtime, Votes, Rating, Metascore and Revenue have correlation with the decision column Success.

We then, also ignore the datas where vote is less than 50 in the movies
"""

indexNames = movies_data[ movies_data['Votes'] <=50 ].index
movies_data.drop(indexNames, inplace=True)

"""Then we have dropped some other unneccessary columns."""

movies_data =movies_data.drop('Rank',axis=1)
movies_data = movies_data.drop('Year',axis=1)
# movies_data = movies_data.drop('Runtime (Minutes)',axis=1)
# movies_data = movies_data.drop('Action',axis=1)
# movies_data = movies_data.drop('Aniimation',axis=1)
# movies_data = movies_data.drop('Biography',axis=1)
# movies_data = movies_data.drop('Comedy',axis=1)
# movies_data = movies_data.drop('Crime',axis=1)
movies_data

"""**Now, we store the features within X (Runtime, Votes, Rating, Metascore and Revenue) and label in Y.**"""

new_set = movies_data
# new_set.drop()
X = new_set.values[:, 0:5]
Y = new_set.values[:,-1]
print(X)
print(Y)

"""**Now we split train and test data in the ratio of 80:20**"""

X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 42)

print(X_train)
print(y_train)
print(X_test)
print(y_test)

print("x_train shape: ", X_train.shape)
print("x_test shape: ", X_test.shape)
print("y_train shape: ", y_train.shape)
print("y_test shape: ", y_test.shape)
print("Number of classes ", len(np.unique(y_train)))

"""This is a function made for plotting confusion matrix

---


"""

import itertools
def plot_confusion_matrix(model, X, y, class_names, file_name,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    y_pred = model.predict(X)
    # Compute confusion matrix
    cnf_matrix  = confusion_matrix(y, y_pred)
    np.set_printoptions(precision=2)
    plt.figure(figsize=(12, 10))

    if normalize:
        cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    plt.imshow(cnf_matrix, interpolation='nearest', cmap=cmap)
    plt.title('confusion matrix')
    plt.colorbar()
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)

    fmt = '.2f' if normalize else 'd'
    thresh = cnf_matrix.max() / 2.
    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):
        plt.text(j, i, format(cnf_matrix[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cnf_matrix[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.savefig(file_name+'.png')
    plt.show()

"""Neccessary packages and libraries for precision, accuracy, f1 score and so on."""

from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt

"""# Decision Tree"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

forest_params = [{'max_depth': list(range(10, 15)), 'max_features': list(range(0,5))}]



dt = DecisionTreeClassifier(criterion='entropy')
clf_for_dt = GridSearchCV(dt, forest_params, cv = 10, scoring='accuracy')

clf_for_dt.fit(X_train, y_train)

print(clf_for_dt.best_params_)
print(clf_for_dt.best_score_)
print("Training accuracy:", clf_for_dt.score(X_train, y_train))
print("Test accuracy", clf_for_dt.score(X_test, y_test))

from sklearn.metrics import confusion_matrix

LABELS = ["Success","Not Successful"]
plot_confusion_matrix(clf_for_dt, X_test, y_test, class_names=LABELS, file_name='ConfussionMatrix', normalize=True)

y_pred2 = clf_for_dt.predict(X_test)

print('Precision: %.3f' % precision_score(y_test, y_pred2,average='micro'))

print('Recall: %.3f' % recall_score(y_test, y_pred2, average='macro'))

print('F1 Score: %.3f' % f1_score(y_test, y_pred2,average='macro'))

from sklearn.metrics import roc_curve
pred_prob_dt = clf_for_dt.predict_proba(X_test)
fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob_dt[:,1], pos_label=1)

from sklearn.metrics import roc_auc_score

# auc scores
auc_score1 = roc_auc_score(y_test, pred_prob_dt[:,1])
print("Auc Score: ",auc_score1)

"""# Random Forest

Accuracy = (TP + TN)/(TP + TN + FP + FN)

  Precision = TP/(TP + FP)

  Recall = TP / ( TP + FN)

Fl score = (2*Precision*Recall)/(Precision + Recall)
"""

random_forest_en =RandomForestClassifier(n_estimators=300)

clf_of_rf = GridSearchCV(random_forest_en, forest_params, cv = 10, scoring='accuracy')

clf_of_rf.fit(X_train, y_train)

print(clf_of_rf.best_params_)
print(clf_of_rf.best_score_)
print("Training accuracy:", clf_of_rf.score(X_train, y_train))
print("Test accuracy", clf_of_rf.score(X_test, y_test))

from sklearn.metrics import confusion_matrix

LABELS = ["Success","Not Successful"]
plot_confusion_matrix(clf_of_rf, X_test, y_test, class_names=LABELS, file_name='ConfussionMatrix', normalize=True)

y_pred1 =clf_of_rf.predict(X_test)

print('Precision: %.3f' % precision_score(y_test, y_pred1,average='micro'))

print('Recall: %.3f' % recall_score(y_test, y_pred1, average='macro'))

print('F1 Score: %.3f' % f1_score(y_test, y_pred1,average='macro'))

pred_prob_rf = clf_of_rf.predict_proba(X_test)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob_rf[:,1], pos_label=1)

# from sklearn.metrics import roc_auc_score

# auc scores
auc_score2 = roc_auc_score(y_test, pred_prob_rf[:,1])
print("Auc Score: ",auc_score2)

"""# Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
clf_of_NB = GaussianNB()
clf_of_NB = clf_of_NB.fit(X_train, y_train)

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
y_pred3 = clf_of_NB.predict(X_train)
y_pred5 = clf_of_NB.predict(X_test)
print('Accuracy Score on train data: ', accuracy_score(y_train, y_pred3))
print('Accuracy Score on the test data: ', accuracy_score(y_test, y_pred5))

from sklearn.metrics import confusion_matrix

LABELS = ["Success","Not Successful"]
plot_confusion_matrix(clf_of_NB, X_test, y_test, class_names=LABELS, file_name='ConfussionMatrix', normalize=True)

print('Precision: %.3f' % precision_score(y_test, y_pred5,average='micro'))

print('Recall: %.3f' % recall_score(y_test, y_pred5, average='macro'))

print('F1 Score: %.3f' % f1_score(y_test, y_pred5,average='macro'))

pred_prob_NB = clf_of_NB.predict_proba(X_test)
fpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob_NB[:,1], pos_label=1)

# from sklearn.metrics import roc_auc_score

# auc scores
auc_score3 = roc_auc_score(y_test, pred_prob_NB[:,1])
print("Auc Score: ",auc_score3)

"""ROC AUC is the area under the ROC curve and is often used to evaluate the ordering quality of two classes of objects by an algorithm.

# AUC Comparison
"""

print("AUC Score for Decision Tree: ",auc_score1)
print("AUC Score for Random Forest: ",auc_score2)
print("AUC Score for Naive Bayes: ",auc_score3)

"""# ROC Comparison"""

random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)

plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Decision Tree')
plt.plot(fpr2, tpr2, linestyle='--',color='green', label='Random Forest')
plt.plot(fpr3, tpr3, linestyle='--',color='red', label='Naive Bayes')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();